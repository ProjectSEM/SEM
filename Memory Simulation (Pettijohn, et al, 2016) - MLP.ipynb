{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pettijohn et al., 2016\n",
    "Here, we are simulating the findings of Pettijon, et al. (2016) where the main finding was that a list of items broken up into multiple events were remembered better as a whole than an equivalent list studied in a single event\n",
    "\n",
    "## Description of the original Experiment\n",
    "Subjects were given a list of 40 words to remember while moving between four locations in physical space, divided into 4 ordered sub-lists of 10 words each. Subjects read one sub-list (10 words) then moved to a new location in space, either in a new room (shift condition) or a new space in the same room (no-shift condition) that was equated for physical distance, and read a second sub-list (10 words). Subjects were then given a distractor task, then asked to recall as many of the words as possible from both tested sub-lists.  Then the procedure was repeated with the second set of sub-lists. People were more accurate when there was a shift (main effect, ANOVA) but there was no effect of boundary X shift interaction (ANOVA, two words before and after a shift)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the simulations\n",
    "Here, we generate a list of $n=20$ items $\\mathbf{x} \\in \\mathbb{R}^d$ by drawing each as a random Gaussian vector $\\mathbf{x}\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ in high (d-)dimensional space, such that each item is approximately orthogonal. We will assume these are either encoded within a single event (simulating the no-switch condition) or in two events (simulating the switch condition) and evaluate the recall of all of the items for both conditions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.9.0\n",
      "Keras      Version: 2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from models import *\n",
    "from tqdm import tnrange\n",
    "from simulations.exp_pettijohn import generate_task, batch\n",
    "\n",
    "sns.set_context('paper', font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train either one or two event models on the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau: 0.1\n"
     ]
    }
   ],
   "source": [
    "# set the parameters, all of them!\n",
    "\n",
    "# SEM parameters\n",
    "df0 = 100\n",
    "scale0 = .2\n",
    "\n",
    "lmda = 1.0  # stickyness parameter\n",
    "alfa = 1.  # concentration parameter\n",
    "\n",
    "f_class = KerasMultiLayerPerceptron\n",
    "f_opts=dict(var_scale0=scale0, var_df0=df0)\n",
    "\n",
    "# create the corrupted memory trace\n",
    "# noise parameters\n",
    "b = 2\n",
    "tau = 0.1\n",
    "print(\"tau: {}\".format(tau))\n",
    "\n",
    "# set the parameters for the Gibbs sampler\n",
    "gibbs_kwargs = dict(\n",
    "    memory_alpha = alfa,\n",
    "    memory_lambda = lmda,\n",
    "    memory_epsilon = np.exp(-11),\n",
    "    b = b,  # re-defined here for completeness\n",
    "    tau = tau,  # ibid\n",
    "    n_samples = 250,\n",
    "    n_burnin = 100,\n",
    "    leave_progress_bar=False,\n",
    ")\n",
    "\n",
    "epsilon_e = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior variance (mode): 0.196078431373\n",
      "Median Feature variance: 0.041616619943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run SEM: 100%|██████████| 2/2 [00:06<00:00,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calibrate the prior\n",
    "x_list_no_switch, x_list_switch = generate_task()\n",
    "\n",
    "mode = df0 * scale0 / (df0 + 2)\n",
    "print(\"Prior variance (mode): {}\".format(mode))\n",
    "print(\"Median Feature variance: {}\".format(np.median(np.var(x_list_no_switch[0], axis=0))))\n",
    "\n",
    "sem_kwargs = dict(\n",
    "    lmda=lmda, alfa=alfa, f_class=f_class,\n",
    "    f_opts=f_opts\n",
    ")\n",
    "sem = SEM(**sem_kwargs)\n",
    "sem.run_w_boundaries(list_events=x_list_switch)\n",
    "print(sem.results.e_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Batch', max=25), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/memory.py:129: RuntimeWarning: divide by zero encountered in log\n",
      "  log_p = p_model + np.log(p_sCRP)\n",
      "simulations/exp_pettijohn.py:121: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  }, index=[batch_number])])\n",
      "Run SEM:   0%|          | 0/1 [00:00<?, ?it/s]                  /anaconda3/envs/sem/lib/python2.7/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.130334). Check your callbacks.\n",
      "  % delta_t_median)\n",
      "Gibbs Sampler:  23%|██▎       | 80/350 [00:09<00:31,  8.69it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tnrange\n",
    "n_batch = 25\n",
    "\n",
    "results = []\n",
    "for ii in tnrange(n_batch, desc='Batch'):\n",
    "    results.append(batch(sem_kwargs, gibbs_kwargs, epsilon_e, ii))\n",
    "results = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results.groupby('Condition').mean()['Overall Acc'].index\n",
    "y = results.groupby('Condition').mean()['Overall Acc']\n",
    "y_err = results.groupby('Condition').std()['Overall Acc'] / np.sqrt(n_batch)\n",
    "plt.figure(figsize=(2, 3))\n",
    "plt.bar(x, y, lw=1, edgecolor='k', facecolor=[0.75,0.75,0.75])\n",
    "plt.errorbar(x, y, y_err, marker='None', lw=2, ls='None', c='k')\n",
    "plt.ylabel('Accuracy')\n",
    "sns.despine()\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "ttest_ind(\n",
    "    results.loc[results.Condition == 'Shift', 'Overall Acc'],\n",
    "    results.loc[results.Condition == 'No-Shift', 'Overall Acc']\n",
    ")\n",
    "plt.savefig('pettijohn_mlp.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = results.copy()\n",
    "res.rename(columns={'Boundary Acc':'Boundary', 'Non-boundary Acc':'Non-boundary', 'Condition': 'Group'}, inplace=True)\n",
    "res = res.melt(id_vars=['Group'], value_vars=['Boundary', 'Non-boundary'], value_name='Accuracy', \n",
    "               var_name='Condition')\n",
    "sns.factorplot(data=res, y='Accuracy', x='Group', hue='Condition', \n",
    "               kind='bar', order=['No-Shift', 'Shift'],\n",
    "               palette=[[0.75,0.75,0.75], [0.5,0.5,0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[results.Condition == 'Shift'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results.loc[results.Condition == 'Shift', 'Reconstruction Segementation']\n",
    "y = results.loc[results.Condition == 'Shift', 'Overall Acc'].values\n",
    "sns.regplot(x, y)\n",
    "y_bar = np.mean(results.loc[results.Condition != 'Shift', 'Overall Acc'])\n",
    "plt.plot([x.min(), x.max()], [y_bar, y_bar], 'k', ls='--')\n",
    "plt.ylabel('Shift Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand the how the parameter space effects the recovered segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = results.loc[results.Condition == 'Shift', 'r2']\n",
    "y = results.loc[results.Condition == 'Shift', 'Overall Acc'].values\n",
    "sns.regplot(x, y)\n",
    "y_bar = np.mean(results.loc[results.Condition != 'Shift', 'Overall Acc'])\n",
    "plt.plot([x.min(), x.max()], [y_bar, y_bar], 'k', ls='--')\n",
    "plt.ylabel('Shift Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y[x == 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
