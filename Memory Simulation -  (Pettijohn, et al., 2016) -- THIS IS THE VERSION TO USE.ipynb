{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pettijohn et al., 2016\n",
    "Here, we are simulating the findings of Pettijon, et al. (2016) where the main finding was that a list of items broken up into multiple events were remembered better as a whole than an equivalent list studied in a single event\n",
    "\n",
    "## Description of the original Experiment\n",
    "Subjects were given a list of 40 words to remember while moving between four locations in physical space, divided into 4 ordered sub-lists of 10 words each. Subjects read one sub-list (10 words) then moved to a new location in space, either in a new room (shift condition) or a new space in the same room (no-shift condition) that was equated for physical distance, and read a second sub-list (10 words). Subjects were then given a distractor task, then asked to recall as many of the words as possible from both tested sub-lists.  Then the procedure was repeated with the second set of sub-lists. People were more accurate when there was a shift (main effect, ANOVA) but there was no effect of boundary X shift interaction (ANOVA, two words before and after a shift)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the simulations\n",
    "Here, we generate a list of $n=20$ items $\\mathbf{x} \\in \\mathbb{R}^d$ by drawing each as a random Gaussian vector $\\mathbf{x}\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ in high (d-)dimensional space, such that each item is approximately orthogonal. We will assume these are either encoded within a single event (simulating the no-switch condition) or in two events (simulating the switch condition) and evaluate the recall of all of the items for both conditions. \n",
    "\n",
    "Free recall is modeled here as the probability density of item $\\mathbf{x}_i$ was in the study list $\\mathbf{x}_{1:n} = \\left \\{\\mathbf{x}_1, ..., \\mathbf{x}_n \\right \\}$. We define the density at any moment in time in terms of the reconstruction process marginalized across time as:\n",
    "\\begin{equation}\n",
    "    k(\\mathbf{x}_i\\in \\mathbf{x}_{1:n}) = \\sum^{n}_{t=1}k(\\mathbf{x}_t = \\mathbf{x}_i) p_t\n",
    "    \\label{eqn:recall_prob}\n",
    "\\end{equation}\n",
    "were $p_t\\propto 1$ defines the prior over time-points and $k(\\mathbf{x}_t)=\\Pr(\\mathbf{x}_t|\\tilde{\\mathbf{x}})$ is a density function. \n",
    "\n",
    "In practice, we do not have a closed form of the densities and instead can only draw samples from the posterior distribution $\\Pr (\\mathbf{x} | \\tilde{\\mathbf{x}})$. However, we can use these samples to estimate the density function with a parametrized kernel density estimator. Let $\\left <\\mathbf{x}_t\\right >_{1:m}$ be $m$ samples of $\\mathbf{x}_t$, then a kernel density estimator $\\hat{k}_{h}$ is\n",
    "\\begin{equation}\n",
    "    \\label{eqn:kde}\n",
    "    \\hat{k}_{h}(\\mathbf{x}_t = \\mathbf{x}) \\equiv \\frac{1}{m}\\sum^{m}_{i=1}\\kappa_h(\\mathbf{x}-\\left <\\mathbf{x}_t\\right >_j)\n",
    "\\end{equation}\n",
    "where $\\kappa_{h}$ is a kernel function (in our case Gaussian) parameterized by a bandwidth $h$. \n",
    "\n",
    "To marginalize across time, we can draw samples from the generative distribution marginalized across time, which is\n",
    "\\begin{equation}\n",
    "    \\mathbf{x} \\sim \\sum^{n}_{t=1}\\Pr (\\mathbf{x}_t = \\mathbf{x}| \\tilde{\\mathbf{x}}_{1:n})p_t\n",
    "\\end{equation}\n",
    "Equivalently, we can draw $m$ samples across all time points, $\\left <\\mathbf{x}_{1:n}\\right >_{1:m}$ and treat these as $n\\times m$ samples $\\left <\\mathbf{x}\\right >_{1:nm}$. As above, we use the samples $\\left <\\mathbf{x}\\right >_{1:nm}$ to generate a density estimate,\\begin{equation}\n",
    "    \\hat{k}_{h}(\\mathbf{x}) \\equiv \\frac{1}{nm}\\sum^{nm}_{i=1}\\kappa_h(\\mathbf{x}, \\left <\\mathbf{x}\\right >_j)\n",
    "    \\label{eqn:kde_maringal}\n",
    "\\end{equation}\n",
    "\n",
    "For the purpose of these simulations, we only care about the average recall across all items within a condition. So, we will define accuracy as\n",
    "\\begin{equation}\n",
    "    \\text{Recall Accuracy} = \\sum^{n}_{i=1}\\hat k_{h}(\\mathbf{x}_i)\n",
    "\\end{equation}\n",
    "For clarity $\\mathbf{x}_i$ refers to the original item vectors, and as such, recall accuracy is defined as the average density of the items evaluated on the posterior density estimate. In practice, all of the calculations use log densities (and as such, the logsumexp approximation is a potential source of error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from models import SEM, KerasSimpleRNN\n",
    "from tqdm import tnrange\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "np.random.seed(1234)\n",
    "sns.set_context('paper', font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train either one or two event models on the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "D = 25\n",
    "x = np.random.randn(N, D)\n",
    "\n",
    "# for our purposes, we need to properly encode the transition function\n",
    "f_opts=dict(n_epochs=100, optimizer='adam', n_hidden1=D, n_hidden2=D,\n",
    "           l2_regularization=0.0, dropout=0.00)\n",
    "event_model = KerasSimpleRNN(D, **f_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Event model error 56.95138182439592\n",
      "Untrained model error 460.39373938197934\n"
     ]
    }
   ],
   "source": [
    "event_model.update_f0(x[0, :])\n",
    "for ii in range(1, N):\n",
    "    event_model.update(x[ii-1, :], x[ii, :])\n",
    "\n",
    "# evaluate the geneartive error of the model\n",
    "x_hat = event_model.run_generative(N)\n",
    "print \"Single Event model error\", np.sum((x - x_hat)**2)\n",
    "\n",
    "# for comparison, the generative error of an untrained model\n",
    "null_model = KerasSimpleRNN(D, **f_opts)\n",
    "x_hat = null_model.run_generative(N)\n",
    "np.sum((x - x_hat)**2)\n",
    "print \"Untrained model error\", np.sum((x - x_hat)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise parameter\n",
    "tau = 1.00\n",
    "x_mem = x + np.random.randn(N, D) * tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recontstruction  functions\n",
    "Here, we define parameters of the reconstrcution common to all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.01  # Transition noise parameter (assumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a reconstruction function that assumes no event boundaries occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_no_boundaries(x_mem, event_model, burn_in=500, n_samples=2000):\n",
    "    x_samples = []\n",
    "\n",
    "    # randomly intialize the sample of x0 from the corruption process\n",
    "    x0 = np.random.randn(N, D) * tau + x_mem\n",
    "\n",
    "    # because the beta and tau are assumed and constant, we can pre-cacluate u \n",
    "    # and lambda for the weighted mean of the posterior over x\n",
    "    u = (1./beta) / (1./beta + 1./tau)  #beta and tau are global variables\n",
    "    lmda = 1.0 / (1./beta + 1./tau)\n",
    "\n",
    "    trials = range(0, N) \n",
    "    for ii in range(n_samples + burn_in):\n",
    "        np.random.shuffle(trials) # randomize the order\n",
    "        for t in trials:\n",
    "\n",
    "        # pull the correct transition function\n",
    "            if t > 1:\n",
    "                def f(X):\n",
    "                    return event_model.predict_next_generative(X)\n",
    "            else:\n",
    "                def f(X):\n",
    "                    return event_model.predict_f0()\n",
    "\n",
    "            # construct the input vector for the model\n",
    "            x_i = x0[:t, :]\n",
    "\n",
    "            # use the likelihood function to estimate a new sample. \n",
    "            # This likelihood function is the product of two gaussians:\n",
    "            #  N(x0_t; f(x0_{1:t-1}), beta_f * I) * N(x0_t; \\tilde x_t , beta_mem * I )\n",
    "            mu_t = u * f(x_i) + (1 - u) * x_mem[t]\n",
    "\n",
    "            # generate a sample from the multivariate normal\n",
    "            x0[t, :] = np.random.multivariate_normal(mu_t.flatten(), lmda * np.eye(D))\n",
    "\n",
    "        if ii >= burn_in:\n",
    "            x_samples.append(x0.copy())    \n",
    "    return np.array(x_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a reconstruction function that assumes a single event boundary halfway through the study list. Here, we assume that the event dynamics are linked to the correct event (This isn't a strong assumption for only two events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_single_boundary(x_mem, event_model_1, event_model_2, \n",
    "                                burn_in=500, n_samples=2000):    \n",
    "    x_samples = []\n",
    "    \n",
    "    # randomly intialize the sample of x0 from the corruption process\n",
    "    x0 = np.random.randn(N, D) * tau + x_mem\n",
    "\n",
    "    # because the beta and tau are assumed and constant, we can pre-cacluate u \n",
    "    # and lambda for the weighted mean of the posterior over x\n",
    "    u = (1./beta) / (1./beta + 1./tau)\n",
    "    lmda = 1.0 / (1./beta + 1./tau)\n",
    "\n",
    "    trials = range(0, N) \n",
    "    for ii in range(n_samples + burn_in):\n",
    "        np.random.shuffle(trials) # randomize the order\n",
    "        for t in trials:\n",
    "\n",
    "            # pull the correct transition function\n",
    "            if t < N/2:\n",
    "                if t > 1:\n",
    "                    def f(X):\n",
    "                        return event_model_1.predict_next_generative(X)\n",
    "                else:\n",
    "                    def f(X):\n",
    "                        return event_model_1.predict_f0()\n",
    "                # construct the input vector for the model\n",
    "                x_i = x0[:t, :]\n",
    "            else:\n",
    "                if t > N/2:\n",
    "                    def f(X):\n",
    "                        return event_model_2.predict_next_generative(X)\n",
    "                else:\n",
    "                    def f(X):\n",
    "                        return event_model_2.predict_f0()\n",
    "                # construct the input vector for the model\n",
    "                x_i = x0[N/2:t, :]\n",
    "\n",
    "            # use the likelihood function to estimate a new sample. \n",
    "            # This likelihood function is the product of two gaussians:\n",
    "            #  N(x0_t; f(x0_{1:t-1}), beta_f * I) * N(x0_t; \\tilde x_t , beta_mem * I )\n",
    "\n",
    "            mu_t = u * f(x_i) + (1 - u) * x_mem[t]\n",
    "\n",
    "            # generate a sample from the multivariate normal\n",
    "            x0[t, :] = np.random.multivariate_normal(mu_t.flatten(), lmda * np.eye(D))\n",
    "\n",
    "        if ii >= burn_in:\n",
    "            x_samples.append(x0.copy())\n",
    "                          \n",
    "    return np.array(x_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to generate a batch including a randomly drawn study list and the two alternatate reconstructions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sample(burn_in=500, n_samples=2000):\n",
    "    \n",
    "    x = np.random.randn(N, D)  # generate a new sample of random scene vectors\n",
    "    \n",
    "    # noise parameter\n",
    "    tau = 1.00\n",
    "    x_mem = x + np.random.randn(N, D) * tau\n",
    "    \n",
    "    ############# estimate a single model for event structre\n",
    "    event = KerasSimpleRNN(D, **f_opts)\n",
    "    event.update_f0(x[0, :])\n",
    "    for ii in range(1, N):\n",
    "        event.update(x[ii-1, :], x[ii, :])\n",
    "    \n",
    "    trials = range(0, N) \n",
    "    beta = 0.01\n",
    "    \n",
    "    x_samples_1e = reconstruct_no_boundaries(x_mem, event, burn_in=burn_in, \n",
    "                                             n_samples=n_samples)\n",
    "\n",
    "    #####################\n",
    "    \n",
    "    # train new event models!\n",
    "    event1 = KerasSimpleRNN(D, **f_opts)\n",
    "    event2 = KerasSimpleRNN(D, **f_opts)\n",
    "    x_e1 = x[:10, :]\n",
    "    event1.update_f0(x_e1[0, :])\n",
    "    for ii in range(1, N/2):\n",
    "        event1.update(x_e1[ii-1, :], x_e1[ii, :])\n",
    "    x_e2 = x[10:, :]\n",
    "    event2.update_f0(x_e2[0, :])\n",
    "    for ii in range(1, N/2):\n",
    "        event2.update(x_e2[ii-1, :], x_e2[ii, :])\n",
    "    \n",
    "    x_samples_2e = reconstruct_single_boundary(x_mem, event_model_1=event1, \n",
    "                                               event_model_2=event2, burn_in=burn_in,\n",
    "                                              n_samples=n_samples)\n",
    "            \n",
    "    return {'StudyList': x, 'Sample No Boundary': x_samples_1e,\n",
    "            'Sample Boundary': x_samples_2e}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517d2d08c1914777a70013801bca96b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run the simluations\n",
    "\n",
    "n_batch = 25\n",
    "burn_in = 500\n",
    "n_samples = 2000\n",
    "\n",
    "results = {ii: batch_sample(burn_in, n_samples) for ii in tnrange(n_batch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the recall given the reconstructed memory samples for each batch\n",
    "\n",
    "def logsumexp_mean(x, axis=0):\n",
    "    return logsumexp(x, axis=0) - np.log(np.shape(x)[axis])\n",
    "\n",
    "hs = np.arange(0.1, 0.51, 0.1)\n",
    "one_event_scores = np.zeros((n_batch, len(hs)))\n",
    "two_event_scores = np.zeros((n_batch, len(hs)))\n",
    "\n",
    "for ii, res in results.iteritems():\n",
    "    ############# Generate density estimates!\n",
    "    if hs is None:\n",
    "        hs = np.arange(0.1, 1.0, 0.01)\n",
    "        \n",
    "    x = res['StudyList']\n",
    "    x_s1e = res['Sample No Boundary']\n",
    "    x_s2e = res['Sample Boundary']\n",
    "\n",
    "\n",
    "    for jj, h in enumerate(hs):\n",
    "        \n",
    "        # estimate a density function for each sample and parameter\n",
    "        k_1e = KernelDensity(bandwidth=h).fit(np.reshape(x_s1e, (-1, D)))\n",
    "        k_2e = KernelDensity(bandwidth=h).fit(np.reshape(x_s2e, (-1, D)))\n",
    "\n",
    "        # average the densities overall the samples (these are log densities, but \n",
    "        # we are averaging over densities) and store\n",
    "        one_event_scores[ii, jj] = logsumexp_mean(k_1e.score_samples(x))\n",
    "        two_event_scores[ii, jj] = logsumexp_mean(k_2e.score_samples(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results!\n",
    "\n",
    "import pandas as pd\n",
    "from seaborn.algorithms import bootstrap\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, gridspec_kw={'width_ratios':[1, 2], 'wspace':0.4},\n",
    "                         figsize=(8, 4))\n",
    "\n",
    "h = 0.4\n",
    "\n",
    "cc = sns.color_palette('Dark2')\n",
    "\n",
    "idx = np.arange(len(hs))[(hs - h)**2 < 0.0001][0]\n",
    "df = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'Condition': ['No Switch'] * one_event_scores.shape[0],\n",
    "        'Recall - log Density': one_event_scores[:, idx]\n",
    "    }),\n",
    "    pd.DataFrame({\n",
    "        'Condition': ['Switch'] * two_event_scores.shape[0],\n",
    "        'Recall - log Density': two_event_scores[:, idx]\n",
    "    })])\n",
    "\n",
    "sns.barplot(data=df, x='Condition', y='Recall - log Density', \n",
    "            ax=axes[0], estimator=logsumexp_mean, palette=cc)\n",
    "axes[0].set_title('Bandwidth = %.1f' % h)\n",
    "\n",
    "ax = axes[1]\n",
    "y = logsumexp_mean(one_event_scores)\n",
    "ax.plot(hs, y, color=cc[0], label='No Boundaries')\n",
    "\n",
    "y = logsumexp_mean(two_event_scores)\n",
    "ax.plot(hs, y, color=cc[1], label='Event Boundary')\n",
    "\n",
    "# use bootstrapping to estimate a confidence interval\n",
    "boot_data = bootstrap(one_event_scores, func=logsumexp_mean)\n",
    "y_er_lb, y_er_ub = sns.utils.ci(boot_data, axis=0)\n",
    "ax.fill_between(hs, y_er_lb, y_er_ub, color=cc[0], alpha=0.50)\n",
    "\n",
    "boot_data = bootstrap(two_event_scores, func=logsumexp_mean)\n",
    "y_er_lb, y_er_ub = sns.utils.ci(boot_data, axis=0)\n",
    "ax.fill_between(hs, y_er_lb, y_er_ub, color=cc[1], alpha=0.50)\n",
    "\n",
    "\n",
    "ax.set_xlabel('Bandwidth')\n",
    "ax.set_ylabel('Recall - log Density')\n",
    "ax.legend()\n",
    "sns.despine(offset=5)\n",
    "\n",
    "plt.savefig('pettijohn.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_one_event = np.zeros((n_batch, N))\n",
    "distances_two_event = np.zeros((n_batch, N))\n",
    "\n",
    "for ii, res in results.iteritems():\n",
    "    ############# Generate density estimates!\n",
    "    if hs is None:\n",
    "        hs = np.arange(0.1, 1.0, 0.01)\n",
    "        \n",
    "    x = res['StudyList']\n",
    "    x_s1e = res['Sample No Boundary']\n",
    "    x_s2e = res['Sample Boundary']\n",
    "    \n",
    "    distances_one_event[ii, :] = np.mean(np.sum((x_s1e  - np.tile(x, (n_samples, 1, 1))) ** 2, axis=2), axis=0)\n",
    "    distances_two_event[ii, :] = np.mean(np.sum((x_s2e  - np.tile(x, (n_samples, 1, 1))) ** 2, axis=2), axis=0)\n",
    "\n",
    "y1 = np.mean(distances_one_event, axis=0)\n",
    "y2 = np.mean(distances_two_event, axis=0)\n",
    "y1_err = np.std(distances_one_event, axis=0)\n",
    "y2_err = np.std(distances_two_event, axis=0)\n",
    "\n",
    "plt.errorbar(range(1, N+1), y1, label='One Event', yerr=y1_err)\n",
    "plt.errorbar(range(1, N+1), y2, label='Two Events', yerr=y2_err)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(r'Avg Dist $||\\mathbf{x}_t -\\left <\\mathbf{x}_t\\right >||$')\n",
    "plt.xlabel(r'Study List Item $\\mathbf{x}_t$')\n",
    "plt.xticks(range(0, 21, 5))\n",
    "\n",
    "_, yub = plt.gca().get_ylim()\n",
    "plt.plot([10.5, 10.5], [0, yub], 'k--')\n",
    "plt.ylim([0, yub])\n",
    "\n",
    "sns.despine(offset=10)\n",
    "plt.savefig('pettijohn_reason.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
