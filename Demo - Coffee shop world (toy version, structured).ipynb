{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example of coffee shop world\n",
    "\n",
    "\n",
    "This is a toy example of the type of data we will have from coffee shop world. Coffeeshop world outputs symbolic sentences, but as of right now, these still need to be hand processed to create embeddings. \n",
    "\n",
    "Here, we create two example events (a poetry reading and a fight) that we will alternate for the SEM model. The strings below represents a raw output of a structured coffee shop world story. We will create two sequences of strings from them, representing each event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_raw = 'BEGIN(Mariko); Sit_down(Mariko,Sarah); Emcee_intro(Olivia,Julian); Poet_performs(Julian); Subject_performs(Mariko,Sarah); Say_goodbye(Mariko,Sarah); END(Mariko). '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fight_raw = 'BEGIN(Silvia); Walk_to_front(Silvia,Nick); Step_in_front(Nick,Silvia); Ignore(Silvia,Nick,coffee); X_stare(Nick,Silvia); Dessert_crumble(Silvia,muffin,Nick); Call_policeman(Pradeep,Silvia); Hate_coffee(Silvia,Nick); END(Nick,Ben).'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we parse the poetry reading into a list of role/filler binding pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('BEGIN', 'verb'), ('Mariko', 'agent')], [('Sit_down', 'verb'), ('Mariko', 'agent'), ('Sarah', 'patient')], [('Emcee_intro', 'verb'), ('Olivia', 'agent'), ('Julian', 'patient')], [('Poet_performs', 'verb'), ('Julian', 'agent')], [('Subject_performs', 'verb'), ('Mariko', 'agent'), ('Sarah', 'patient')], [('Say_goodbye', 'verb'), ('Mariko', 'agent'), ('Sarah', 'patient')], [('END', 'verb'), ('Mariko', 'agent')]]\n"
     ]
    }
   ],
   "source": [
    "roles = [('verb'), ('agent'), ('patient')]\n",
    "\n",
    "# parse the string for sentences, split at ;\n",
    "poetry_symb = poetry_raw.split('; ')\n",
    "for ii, s in enumerate(poetry_symb):\n",
    "    # split the string and remove the structure markers\n",
    "    s = s.replace('(', \" \").replace(')', ' ').replace(',', ' ').replace('.','').split()\n",
    "    \n",
    "    # attach the role bindinds\n",
    "    s = [(s[jj], roles[jj]) for jj in range(len(s))]\n",
    "    poetry_symb[ii] = s    \n",
    "\n",
    "print poetry_symb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeate the same for the fight event (this requires a little custom code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('BEGIN', 'verb'), ('Silvia', 'agent')], [('Walk_to_front', 'verb'), ('Silvia', 'agent'), ('Nick', 'patient')], [('Step_in_front', 'verb'), ('Nick', 'agent'), ('Silvia', 'patient')], [('Ignore', 'verb'), ('Silvia', 'agent'), ('Nick', 'patient')], [('Order_drink', 'verb'), ('Silvia', 'agent'), ('coffee', 'patient')], [('X_stare', 'verb'), ('Nick', 'agent'), ('Silvia', 'patient')], [('Dessert_crumble', 'verb'), ('Silvia', 'agent'), ('muffin', 'instrument'), ('Nick', 'patient')], [('Call_policeman', 'verb'), ('Pradeep', 'agent'), ('Silvia', 'patient')], [('Hate_coffee', 'verb'), ('Silvia', 'agent'), ('Nick', 'patient')], [('END', 'verb'), ('Nick', 'agent'), ('Ben', 'patient')]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roles = [('verb'), ('agent'), ('patient')]\n",
    "\n",
    "# parse the string for sentences, split at ;\n",
    "fight_symb = fight_raw.split('; ')\n",
    "for ii, s in enumerate(fight_symb):\n",
    "    # split the string and remove the structure markers\n",
    "    s = s.replace('(', \" \").replace(')', ' ').replace(',', ' ').replace('.','').split()\n",
    "    \n",
    "    # attach the role bindinds\n",
    "    # break the ignore sentence into two\n",
    "    if s[0] == 'Ignore':\n",
    "        s = [[(s[0], 'verb'), (s[1], 'agent'), (s[2], 'patient')], \n",
    "             [('Order_drink', 'verb'), (s[1], 'agent'), (s[3], 'patient')]]\n",
    "    elif s[0] == 'Dessert_crumble':\n",
    "        s = [(s[0], 'verb'), (s[1], 'agent'), (s[2], 'instrument'), (s[3], 'patient')]\n",
    "    else:\n",
    "        s = [(s[jj], roles[jj]) for jj in range(len(s))]\n",
    "        \n",
    "    fight_symb[ii] = s    \n",
    "    \n",
    "_fight_symb = []\n",
    "for f in fight_symb:\n",
    "    if type(f[0]) == list:\n",
    "        for f0 in f:\n",
    "            _fight_symb.append(f0)\n",
    "    else:\n",
    "        _fight_symb.append(f)\n",
    "fight_symb = _fight_symb\n",
    "\n",
    "print fight_symb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy example, these two events are very different and have very, very little overlap between them. This is ideal for a toy demonstration (at least a first one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BEGIN', 'END'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the intersection of filler words between the two events?\n",
    "set([b[0] for s in poetry_symb for b in s]).intersection([b[0] for s in fight_symb for b in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we embed these into vectors, we need a library of fillers and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['instrument', 'verb', 'patient', 'agent'])\n",
      "set(['Sarah', 'Call_policeman', 'muffin', 'Silvia', 'Pradeep', 'Nick', 'Ben', 'BEGIN', 'Subject_performs', 'Olivia', 'Say_goodbye', 'Emcee_intro', 'Sit_down', 'Dessert_crumble', 'Order_drink', 'Hate_coffee', 'coffee', 'END', 'Poet_performs', 'Ignore', 'Walk_to_front', 'Julian', 'Step_in_front', 'X_stare', 'Mariko'])\n"
     ]
    }
   ],
   "source": [
    "role_library = set([b[1] for s in fight_symb for b in s] + [b[1] for s in poetry_symb for b in s])\n",
    "filler_library = set([b[0] for s in fight_symb for b in s] + [b[0] for s in poetry_symb for b in s])\n",
    "\n",
    "\n",
    "print role_library\n",
    "print filler_library\n",
    "\n",
    "from opt import plate_formula, embed, encode, decode, embed_onehot\n",
    "\n",
    "# figure out how many dimensions we need\n",
    "n = len(role_library) + len(filler_library);     # vocabulary size\n",
    "k = 8;      # maximum number of terms to be combined\n",
    "err = 0.01; # error probability\n",
    "d = plate_formula(n, k, err);\n",
    "\n",
    "# create a library of vectors\n",
    "role_dict = {r: embed(1, d) for r in role_library}\n",
    "filler_dict = {f: embed_onehot(1, d) for f in filler_library}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this version, as a test, we are not going to use role/filler bindings. This is because the convolution opperation alters the vector space, making models harder to train. That is, if we pick guassian random unit vectors as our embedding space, the product of the convolution is does not have a mean of 0 and std of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role    mean = 0.00, std = 0.98\n",
      "Filler  mean = 0.00, std = 0.05\n",
      "Encoded mean = 0.00, std = 0.05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "r = role_dict[role_dict.keys()[0]]\n",
    "print \"Role    mean = %.2f, std = %.2f\" % (np.mean(r), np.std(r))\n",
    "f = filler_dict[filler_dict.keys()[0]]\n",
    "print \"Filler  mean = %.2f, std = %.2f\" % (np.mean(f), np.std(f))\n",
    "\n",
    "e = encode(f, r)\n",
    "print \"Encoded mean = %.2f, std = %.2f\" % (np.mean(e), np.std(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using circular convolution to encode the role filler binding blows out the variance. This will make training difficult, and requires more investigation on our part before we are ready to go ahead (at least, nothing tried has worked so far). Specifically, we don't know how to best normalize these vectors for machine learnign without distroying the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create the structured representation of the sentences for each event\n",
    "def embed_event(event_symb):\n",
    "    event_embedded = []\n",
    "    for s in event_symb:\n",
    "        X0 = np.zeros((1, d))\n",
    "        for filler, role in s:\n",
    "            X0 += encode(role_dict[role], filler_dict[filler]) + filler_dict[filler]\n",
    "        event_embedded.append(X0)\n",
    "    return np.concatenate(event_embedded)\n",
    "\n",
    "poetry_embedded = embed_event(poetry_symb)\n",
    "fight_embedded = embed_event(fight_symb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate to create the sequence of events [Poetry, Fight, Poetry, Fight]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 421) (34,)\n"
     ]
    }
   ],
   "source": [
    "# okay! create the events for segmentation\n",
    "n_p = poetry_embedded.shape[0]\n",
    "n_t = fight_embedded.shape[0]\n",
    "X_train = np.concatenate([poetry_embedded, fight_embedded, poetry_embedded, fight_embedded])\n",
    "y_train = np.concatenate([np.zeros(n_p), np.ones(n_t), np.zeros(n_p), np.ones(n_t)])\n",
    "print X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling tends to improve segmentation\n",
    "N, D  = X_train.shape\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = scale(X_train)\n",
    "distances = [2X_train[ii, :] - X_train[ii+1, :]) for ii in range(N-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from models import SEM, KerasSRN, KerasGRU\n",
    "from opt.utils import evaluate\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# scaling tends to improve segmentation\n",
    "X_train = scale(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior variance (mode): 1.66666666667\n",
      "Median Feature variance: 1.0\n"
     ]
    }
   ],
   "source": [
    "# set the prior over the variance\n",
    "scale0 = 5.0\n",
    "df0 = 0.4\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# plot the prior of the variance vs an empirical estimate\n",
    "df0 = 10\n",
    "scale0 = 2.0\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "mode = df0 * scale0 / (df0 + 2)\n",
    "print(\"Prior variance (mode): {}\".format(mode))\n",
    "print(\"Median Feature variance: {}\".format(np.median(np.var(X_train, axis=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters for the models\n",
    "_, D = X_train.shape\n",
    "\n",
    "t_horizon = 2\n",
    "\n",
    "# specify the model architecture (makes a big difference! especially the training parameters)\n",
    "f_class = KerasSRN\n",
    "f_opts = dict(var_scale0=scale0, var_df0=df0,\n",
    "              t=t_horizon, n_epochs=100, l2_regularization=0.1, dropout=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 10\n",
      "alpha = 1.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "run() got an unexpected keyword argument 'list_event_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d9a60f919e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'alpha ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOmega\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Adjusted Rand Index:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nicholasfranklin/OneDrive/Projects/SEM/opt/utils.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(X, y, Omega, K, number, save, list_event_boundaries)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0msem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_event_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_event_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: run() got an unexpected keyword argument 'list_event_tokens'"
     ]
    }
   ],
   "source": [
    "Omega = {\n",
    "    'lmda': lmda,  # Stickyness (prior)\n",
    "    'alfa': alfa, # Concentration parameter (prior)\n",
    "    'f_class': f_class,\n",
    "    'f_opts': f_opts\n",
    "}\n",
    "print 'lambda =', lmda\n",
    "print 'alpha =', alfa\n",
    "\n",
    "sem, r = evaluate(X_train, y_train, Omega)\n",
    "print \"Adjusted Rand Index:\", r\n",
    "plt.figure(figsize=(6, 3.0))\n",
    "plt.plot(post)\n",
    "plt.gca().set_ylabel('Posterior cluster probability')\n",
    "plt.gca().set_xlabel('Time')\n",
    "plt.show()\n",
    "tf.Session().close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation with bag of words is well above chance. Notice that the \"fight\" events are better recovered than the \"poetry\" events. This reflects a different degree of structure within them as the fight events mostly reuses two actors.\n",
    "\n",
    "\n",
    "For comparison, let's compare at the max posterior cluster to the true event labels are shown below (the colors are not consistent between the two due to label switching, which is not important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 4))\n",
    "\n",
    "max_post = np.zeros(post.shape)\n",
    "for t in range(post.shape[0]):\n",
    "    max_post[t, :] = post[t, :] == post[t, :].max()\n",
    "axes[0].plot(max_post)\n",
    "axes[0].set_title('SEM max a posterior cluster')\n",
    "\n",
    "y_clust = np.zeros((y_train.shape[0], 2))\n",
    "for ii, y0 in enumerate(y_train):\n",
    "    y_clust[ii, int(y0)] = 1.0\n",
    "axes[1].plot(y_clust)\n",
    "axes[1].set_title('True Clusters')\n",
    "axes[1].set_xlabel('Time')\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very important that both the prior and the posterior contribute to the clustering process. For this to happen, the scale of the differences between the different hypothesis in terms of log-likelihood and log-prior not be drastically different. For example, if the difference likelihood between two event models is ~100 and the difference between the same two is ~5 in the log-prior than the likelihood will dominate the clustering. \n",
    "\n",
    "This scaling can be controled by the $\\beta$ parameter, that is, the likehood noise parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, axes = plt.subplots(2, 1)\n",
    "\n",
    "axes[0].plot(log_lik)\n",
    "axes[0].set_title('Log Likelihoods')\n",
    "\n",
    "axes[1].plot(log_prior)\n",
    "axes[1].set_title('Log Priors')\n",
    "axes[1].set_xlabel('Time')\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction errors as a function of event boundaries\n",
    "import seaborn as sns\n",
    "pe_nonbound = []\n",
    "pe_bound = []\n",
    "for ii, y0 in enumerate(y_train[1:], start=1):\n",
    "    # as an approximation, we consider an event boundary when the event type changes\n",
    "    if y_train[ii - 1] == y0:\n",
    "        pe_nonbound.append(pe[ii])\n",
    "    else:\n",
    "        pe_bound.append(pe[ii])\n",
    "\n",
    "        \n",
    "# Are PEs larger for event boundaries? t-test with unequal variance/sample size:\n",
    "print \"Mean PE at Event Boundaries:   \", np.mean(pe_bound)\n",
    "print \"Mean PE for all other scences: \", np.mean(pe_nonbound)\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "t, p = ttest_ind(pe_bound, pe_nonbound, equal_var=False)\n",
    "n1 = len(pe_bound)\n",
    "n2 = len(pe_nonbound)\n",
    "s1 = np.var(pe_bound)\n",
    "s2 = np.var(pe_nonbound)\n",
    "df = ((s1/n1 + s2/n2)**2) / ( ((s1/n1)**2)/(n1-1)  + ((s2/n2)**2)/(n2-1) )\n",
    "\n",
    "print \"t(%.1f) = %.2f; p = %g\" % (df, t, p)\n",
    "        \n",
    "# Calculate the R.O.C curve using the PEs to classify event boundaries\n",
    "\n",
    "# fit a logistic model, boundary ~ PE\n",
    "import statsmodels.api as sm\n",
    "X = np.reshape(np.concatenate([pe_bound, pe_nonbound]), (-1, 1))\n",
    "X = sm.add_constant(X)\n",
    "y = np.reshape(np.concatenate([np.zeros(len(pe_bound)),\n",
    "                    np.ones(len(pe_nonbound))]), (-1, 1))\n",
    "res = sm.Logit(y, X).fit(disp=0)\n",
    "# Evaluate logistic model with ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "y_hat = res.predict(X)\n",
    "fpr, tpr, thr = roc_curve(y, y_hat)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        \n",
    "with sns.axes_style('ticks'):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8.2, 4), gridspec_kw=dict(wspace=0.2))  \n",
    "    \n",
    "    # plot the distribution of prediction errors\n",
    "    ax = axes[0]\n",
    "    sns.distplot(pe_bound, ax=ax, label='Boundary')\n",
    "    sns.distplot(pe_nonbound, ax=ax, label='Non-Boundary')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Prediction Error (Abs Euclidean dist.)')\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_title('Distribution of PEs')\n",
    "\n",
    "    # plot the ROC curve\n",
    "    ax = axes[1]\n",
    "    lw = 2\n",
    "    ax.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('R.O.C. curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pe)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Prediction Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
